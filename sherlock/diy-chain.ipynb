{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pathlib import Path\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "import re\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "TOKEN_LIMIT = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AzureChatOpenAI(deployment_name=\"gpt4-32k\", model_name=\"gpt-4-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nmain.py\\nThe issue in main.py is in the following line:\\n`node_queue.put((-min_node.next.val, counter, min_node.next))`\\nThe problem is that the value is being negated, which causes the order of the elements in the priority queue to be incorrect. To fix this issue, we need to remove the negation and change the line to:\\n`node_queue.put((min_node.next.val, counter, min_node.next))`\\n']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_text_by_path(\n",
    "    path: Path,\n",
    "    root_dir=Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/src\"),\n",
    "):\n",
    "    with open(root_dir / path, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def format_code_with_path(path: Path, content: str):\n",
    "    return f\"{path}\\n{content}\\n\"\n",
    "\n",
    "\n",
    "def extract_path_from_answer(\n",
    "    answer: str,\n",
    "):  # yolo hardcode\n",
    "    # Get all text that is wrapped in ***\n",
    "    path = re.findall(r\"\\*\\*\\*(.*?)\\*\\*\\*\", answer)\n",
    "    return path\n",
    "\n",
    "\n",
    "def extract_notes_from_answer(answer: str):\n",
    "    # Get all text that is wrapped in ***\n",
    "    notes = re.findall(r\"\\*\\*\\*(.*?)\\*\\*\\*\", answer, re.MULTILINE | re.DOTALL)\n",
    "    return notes\n",
    "\n",
    "\n",
    "test = \"\"\"\n",
    "***\n",
    "main.py\n",
    "The issue in main.py is in the following line:\n",
    "`node_queue.put((-min_node.next.val, counter, min_node.next))`\n",
    "The problem is that the value is being negated, which causes the order of the elements in the priority queue to be incorrect. To fix this issue, we need to remove the negation and change the line to:\n",
    "`node_queue.put((min_node.next.val, counter, min_node.next))`\n",
    "***\n",
    "\"\"\"\n",
    "\n",
    "extract_notes_from_answer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ISSUE = \"\"\"\n",
    "## Why\n",
    "Main.py do not generate correct output when I run `python main.py`\n",
    "\n",
    "\n",
    "## What\n",
    "Expected output:\n",
    "[1, 1, 2, 3, 4, 4, 5, 6]\n",
    "\n",
    "Got output:\n",
    "[1, 4, 5, 1, 3, 4, 2, 6]\n",
    "\"\"\"\n",
    "\n",
    "TEST_DIRECTORY = \"\"\"\n",
    "    ./simple_repo/src/main.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe bug report is:\\n```\\ntest.py don't run\\n```\\n\\nHere's a list of files in the repository:\\n```\\n./test.py\\n```\\n\\nPlease suggest which files we should investigate to help identify the source of the issue. If you believe that examining files would not be helpful in this case, please respond with an empty list.\\n\\nFor each file you suggest, please provide a brief rationale (e.g. it contains relevant code, it's connected to the affected feature, etc.) and limit your suggestion to a maximum of 32000 tokens.\\n\\nResponse in following format:\\n\\n***file1.py***\\nWhy: It contains relevant code\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RELATED_FILES_TEMPLATE = \"\"\"\n",
    "The bug report is:\n",
    "```\n",
    "{issue}\n",
    "```\n",
    "\n",
    "Here's a list of files in the repository:\n",
    "```\n",
    "{filepaths}\n",
    "```\n",
    "\n",
    "Please suggest which files we should investigate to help identify the source of the issue. If you believe that examining files would not be helpful in this case, please respond with an empty list.\n",
    "\n",
    "For each file you suggest, please provide a brief rationale (e.g. it contains relevant code, it's connected to the affected feature, etc.) and limit your suggestion to a maximum of {token_limit} tokens.\n",
    "\n",
    "Response in following format:\n",
    "\n",
    "***file1.py***\n",
    "Why: It contains relevant code\n",
    "\"\"\"\n",
    "\n",
    "related_files_prompt = PromptTemplate(\n",
    "    input_variables=[\"issue\", \"filepaths\", \"token_limit\"],\n",
    "    template=RELATED_FILES_TEMPLATE,\n",
    ")\n",
    "# test\n",
    "related_files_prompt.format(\n",
    "    issue=\"test.py don't run\", filepaths=\"./test.py\", token_limit=TOKEN_LIMIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***./simple_repo/src/main.py***\n",
      "Why: It contains the main script that generates the output and where the issue is most likely to be found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./simple_repo/src/main.py']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_related_files(issue_str, filepaths_str):\n",
    "    # Test with test issue\n",
    "    human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=related_files_prompt,\n",
    "    )\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "    ans = chain.run(issue=issue_str, filepaths=filepaths_str, token_limit=TOKEN_LIMIT)\n",
    "    print(ans)\n",
    "    return extract_path_from_answer(ans)\n",
    "\n",
    "\n",
    "find_related_files(TEST_ISSUE, TEST_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe received an issue report with the following description:\\n```\\ntest.py don\\'t run\\n```\\nWe\\'ve identified the following files in the codebase that may be related to the issue:\\n```\\nprint(\\'hello world\")\\n```\\nPlease read the relevant code and take some notes that will help us plan our code commits to resolve the issue. Specifically, we\\'d like you to note what is going wrong and suggest how we can fix the problem mentioned in issue within a maximum of 32000 tokens.\\n\\nFormat for note of each file:\\n***\\nfile1.py\\nnote for file1.py\\n***\\n***\\nfile2.py\\nnote for file2.py\\n***\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "READ_CODE_TEMPLATE = \"\"\"\n",
    "We received an issue report with the following description:\n",
    "```\n",
    "{issue}\n",
    "```\n",
    "We've identified the following files in the codebase that may be related to the issue:\n",
    "```\n",
    "{code}\n",
    "```\n",
    "Please read the relevant code and take some notes that will help us plan our code commits to resolve the issue. Specifically, we'd like you to note what is going wrong and suggest how we can fix the problem mentioned in issue within a maximum of {token_limit} tokens.\n",
    "\n",
    "Format for note of each file:\n",
    "***\n",
    "file1.py\n",
    "note for file1.py\n",
    "***\n",
    "***\n",
    "file2.py\n",
    "note for file2.py\n",
    "***\n",
    "\"\"\"\n",
    "\n",
    "read_code_prompt = PromptTemplate(\n",
    "    input_variables=[\"issue\", \"code\", \"token_limit\"],\n",
    "    template=READ_CODE_TEMPLATE,\n",
    ")\n",
    "# test\n",
    "read_code_prompt.format(\n",
    "    issue=\"test.py don't run\", code=\"print('hello world\\\")\", token_limit=TOKEN_LIMIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'main.py\\nfrom queue import PriorityQueue\\nfrom typing import List, Optional, Tuple\\n\\n\\nclass ListNode:\\n    def __init__(self, val=0, next=None):\\n        self.val = val\\n        self.next = next\\n\\n\\nclass Solution:\\n    def mergeKLists(self, lists: List[Optional[ListNode]]) -> Optional[ListNode]:\\n        if not lists:\\n            return None\\n        node_queue: PriorityQueue[Tuple[int, int, ListNode]] = PriorityQueue(len(lists))\\n        # Use counter to diff same value nodes.\\n        counter = 1\\n        for node in lists:\\n            if node:\\n                node_queue.put((node.val, counter, node))\\n                counter += 1\\n\\n        dummy_node = ListNode(0, None)\\n        current = dummy_node\\n\\n        while not node_queue.empty():\\n            _, _, min_node = node_queue.get()\\n            current.next = min_node\\n            current = current.next\\n            if min_node.next:\\n                node_queue.put((-min_node.next.val, counter, min_node.next))\\n                counter += 1\\n\\n        return dummy_node.next\\n\\n\\ntest_list1 = ListNode(1, ListNode(4, ListNode(5)))\\ntest_list2 = ListNode(1, ListNode(3, ListNode(4)))\\ntest_list3 = ListNode(2, ListNode(6))\\n\\nans = Solution().mergeKLists([test_list1, test_list2, test_list3])\\n\\ncurrent = ans\\nans_list = []\\nwhile current:\\n    ans_list.append(current.val)\\n    current = current.next\\n\\nprint(ans_list)\\n\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"\"\"***main.py***\n",
    "\n",
    "Why: The bug report specifically mentions `main.py` as the source of the incorrect output. It is likely that the code responsible for generating the output is located in this file.\"\"\"\n",
    "\n",
    "\"\\n\".join(\n",
    "    [\n",
    "        format_code_with_path(p, read_text_by_path(p))\n",
    "        for p in extract_path_from_answer(test)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 if the error persists..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "main.py\n",
      "The issue in main.py is with the following line:\n",
      "`node_queue.put((-min_node.next.val, counter, min_node.next))`\n",
      "Here, the negative sign before the value leads to the wrong order of nodes in the PriorityQueue. To fix the issue, we need to remove the negative sign:\n",
      "`node_queue.put((min_node.next.val, counter, min_node.next))`\n",
      "***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nmain.py\\nThe issue in main.py is with the following line:\\n`node_queue.put((-min_node.next.val, counter, min_node.next))`\\nHere, the negative sign before the value leads to the wrong order of nodes in the PriorityQueue. To fix the issue, we need to remove the negative sign:\\n`node_queue.put((min_node.next.val, counter, min_node.next))`\\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with test issue\n",
    "\n",
    "\n",
    "def read_code(issue_str, code_files_strs, code_file_path):\n",
    "    human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=read_code_prompt,\n",
    "    )\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "    concat_code = \"\\n\".join(\n",
    "        [\n",
    "            format_code_with_path(p, read_text_by_path(p, code_file_path))\n",
    "            for p in code_files_strs\n",
    "        ]\n",
    "    )\n",
    "    ans = chain.run(\n",
    "        issue=issue_str,\n",
    "        code=concat_code,\n",
    "        token_limit=TOKEN_LIMIT,\n",
    "    )\n",
    "    print(ans)\n",
    "    return extract_notes_from_answer(ans)\n",
    "\n",
    "\n",
    "read_code(\n",
    "    TEST_ISSUE,\n",
    "    [\"main.py\"],\n",
    "    Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/src\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nmain.py\\nThe issue in main.py is in the following line:\\n`node_queue.put((-min_node.next.val, counter, min_node.next))`\\nThe problem is that the value is being negated, which causes the order of the elements in the priority queue to be incorrect. To fix this issue, we need to remove the negation and change the line to:\\n`node_queue.put((min_node.next.val, counter, min_node.next))`\\n']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHello, we have investigated the reported issue and would like to propose a solution. Here are the notes we gathered while examining the repository:\\n\\nprint(\\'hello world\")\\n\\nThe original issue can be found here:\\ntest.py don\\'t run\\n\\nPlease provide the following information for the pull request in a valid serialized JSON string (readable by json.loads()), and make sure to wrap the result JSON with the delimiter \"***\":\\n\\nPull request description should be in markdown format with the following sections:\\n    - Section \"Why\": A brief description of the issue, and why it happened\\n    - Section \"What\": A brief description of the proposed solution\\n\\nKindly ensure that the necessary files are specified for each commit. Note that folders will be created automatically, so there is no need to make separate commits for them. Please limit your response to a maximum of 32000 tokens.\\n\\n***\\n{\\n    \"title\": \"Pull request title\",\\n    \"description\": \"Pull request description in markdown format that follows the format above\",\\n    \"commits\": [\\n        {\\n            \"commit_message\": \"commit1\",\\n            \"commit_files\": [\"file_path1\", \"file_path2\"]\\n        },\\n        {\\n            \"commit_message\": \"commit2\",\\n            \"commit_files\": [\"file_path3\", \"file_path4\"]\\n        }\\n    ]\\n}\\n***\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLAN_PR_TEMPLATE = \"\"\"\n",
    "Hello, we have investigated the reported issue and would like to propose a solution. Here are the notes we gathered while examining the repository:\n",
    "\n",
    "{notes}\n",
    "\n",
    "The original issue can be found here:\n",
    "{issue}\n",
    "\n",
    "Please provide the following information for the pull request in a valid serialized JSON string (readable by json.loads()), and make sure to wrap the result JSON with the delimiter \"***\":\n",
    "\n",
    "Pull request description should be in markdown format with the following sections:\n",
    "    - Section \"Why\": A brief description of the issue, and why it happened\n",
    "    - Section \"What\": A brief description of the proposed solution\n",
    "\n",
    "Kindly ensure that the necessary files are specified for each commit. Note that folders will be created automatically, so there is no need to make separate commits for them. Please limit your response to a maximum of {token_limit} tokens.\n",
    "\n",
    "***\n",
    "{{\n",
    "    \"title\": \"Pull request title\",\n",
    "    \"description\": \"Pull request description in markdown format that follows the format above\",\n",
    "    \"commits\": [\n",
    "        {{\n",
    "            \"commit_message\": \"commit1\",\n",
    "            \"commit_files\": [\"file_path1\", \"file_path2\"]\n",
    "        }},\n",
    "        {{\n",
    "            \"commit_message\": \"commit2\",\n",
    "            \"commit_files\": [\"file_path3\", \"file_path4\"]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "***\n",
    "\n",
    "\"\"\"\n",
    "plan_pr_prompt = PromptTemplate(\n",
    "    input_variables=[\"notes\", \"issue\", \"token_limit\"],\n",
    "    template=PLAN_PR_TEMPLATE,\n",
    ")\n",
    "# test\n",
    "plan_pr_prompt.format(\n",
    "    issue=\"test.py don't run\", notes=\"print('hello world\\\")\", token_limit=TOKEN_LIMIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "{\n",
      "    \"title\": \"Fix incorrect output in main.py\",\n",
      "    \"description\": \"## Why\\nMain.py did not generate the correct output when running `python main.py` due to an issue with the negation of the value in the priority queue.\\n\\n## What\\nThe proposed solution is to remove the negation, which fixes the ordering of the elements in the priority queue and provides the expected output.\",\n",
      "    \"commits\": [\n",
      "        {\n",
      "            \"commit_message\": \"Remove negation in priority queue\",\n",
      "            \"commit_files\": [\"main.py\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "***\n",
      "{'title': 'Fix incorrect output in main.py', 'description': '## Why\\nMain.py did not generate the correct output when running `python main.py` due to an issue with the negation of the value in the priority queue.\\n\\n## What\\nThe proposed solution is to remove the negation, which fixes the ordering of the elements in the priority queue and provides the expected output.', 'commits': [{'commit_message': 'Remove negation in priority queue', 'commit_files': ['main.py']}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Fix incorrect output in main.py',\n",
       " 'description': '## Why\\nMain.py did not generate the correct output when running `python main.py` due to an issue with the negation of the value in the priority queue.\\n\\n## What\\nThe proposed solution is to remove the negation, which fixes the ordering of the elements in the priority queue and provides the expected output.',\n",
       " 'commits': [{'commit_message': 'Remove negation in priority queue',\n",
       "   'commit_files': ['main.py']}]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Test with test issue\n",
    "def extract_pr_from_answer(answer: str):\n",
    "    # Get all text that is wrapped in ***\n",
    "    raw = re.findall(r\"\\*\\*\\*(.*?)\\*\\*\\*\", answer, re.MULTILINE | re.DOTALL)\n",
    "    # convert text to dict\n",
    "    try:\n",
    "        if raw[0]:\n",
    "            return json.loads(raw[0])\n",
    "    except IndexError:\n",
    "        return json.loads(answer)  # HACK ai sometime don't respect delimiter\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def plan_pr(issue_str, notes):\n",
    "    human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=plan_pr_prompt,\n",
    "    )\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "    ans = chain.run(\n",
    "        issue=issue_str,\n",
    "        notes=\"\\n\".join(notes),\n",
    "        token_limit=TOKEN_LIMIT,\n",
    "    )\n",
    "    print(ans)\n",
    "    print(extract_pr_from_answer(ans))\n",
    "    return extract_pr_from_answer(ans)\n",
    "\n",
    "\n",
    "plan_pr(\n",
    "    TEST_ISSUE,\n",
    "    [\n",
    "        \"\"\"\n",
    "***\n",
    "main.py\n",
    "The issue in main.py is in the following line:\n",
    "`node_queue.put((-min_node.next.val, counter, min_node.next))`\n",
    "The problem is that the value is being negated, which causes the order of the elements in the priority queue to be incorrect. To fix this issue, we need to remove the negation and change the line to:\n",
    "`node_queue.put((min_node.next.val, counter, min_node.next))`\n",
    "***\n",
    "\"\"\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively walk through the directory and return a list of all files and their relative paths\n",
    "# With the given globs\n",
    "def walk_dir_for_path(dir_path: Path, globs):\n",
    "    files = []\n",
    "    for path in dir_path.rglob(\"*\"):\n",
    "        if path.is_file():\n",
    "            if any([path.match(glob) for glob in globs]):\n",
    "                files.append(path.relative_to(dir_path))\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_code_files_path(root_dir: Path):\n",
    "    return walk_dir_for_path(\n",
    "        root_dir, [\"*.py\", \"*.js\", \"*.ts\", \"*.html\", \"*.css\", \"*.go\", \"*.tsx\", \"*.jsx\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def run_chain(issue_path: Path, code_path: Path):\n",
    "    issue_str = read_text_by_path(issue_path)\n",
    "    code_file_list = \"\\n\".join(\n",
    "        map(lambda x: x.as_posix(), get_code_files_path(code_path))\n",
    "    )\n",
    "    print(\"### Searching for related files ###\")\n",
    "    related_file = find_related_files(issue_str, code_file_list)\n",
    "    print(\"### Analysis code in related file ###\")\n",
    "    notes_on_code = read_code(issue_str, related_file, code_path)\n",
    "    print(\"### Writing first draft of the pull request ###\")\n",
    "    suggested_pr = plan_pr(issue_str, notes_on_code)\n",
    "\n",
    "    return {\n",
    "        \"related_file\": related_file,\n",
    "        \"notes\": notes_on_code,\n",
    "        \"suggested_pr\": suggested_pr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Searching for related files ###\n",
      "***main.py***\n",
      "Why: It contains the relevant code and is the only file in the repository, so the issue must be in this file.\n",
      "### Analysis code in related file ###\n",
      "***\n",
      "main.py\n",
      "The issue is with this line of code:\n",
      "`node_queue.put((-min_node.next.val, counter, min_node.next))`\n",
      "\n",
      "The negative sign before `min_node.next.val` is causing the PriorityQueue to be sorted in descending order. To fix this issue, remove the negative sign and change the line of code to:\n",
      "`node_queue.put((min_node.next.val, counter, min_node.next))`\n",
      "***\n",
      "### Writing first draft of the pull request ###\n",
      "***\n",
      "{\n",
      "    \"title\": \"Fix PriorityQueue sorting in main.py\",\n",
      "    \"description\": \"## Why\\n\\nMain.py do not generate correct output when running `python main.py` due to incorrect sorting in the PriorityQueue.\\n\\n## What\\n\\nThis PR fixes the issue by removing the negative sign before `min_node.next.val`, ensuring that the PriorityQueue is sorted in ascending order.\",\n",
      "    \"commits\": [\n",
      "        {\n",
      "            \"commit_message\": \"Fix PriorityQueue sorting in main.py\",\n",
      "            \"commit_files\": [\"main.py\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "***\n",
      "{'title': 'Fix PriorityQueue sorting in main.py', 'description': '## Why\\n\\nMain.py do not generate correct output when running `python main.py` due to incorrect sorting in the PriorityQueue.\\n\\n## What\\n\\nThis PR fixes the issue by removing the negative sign before `min_node.next.val`, ensuring that the PriorityQueue is sorted in ascending order.', 'commits': [{'commit_message': 'Fix PriorityQueue sorting in main.py', 'commit_files': ['main.py']}]}\n"
     ]
    }
   ],
   "source": [
    "test_pr = run_chain(\n",
    "    issue_path=Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/issues/1.md\"),\n",
    "    code_path=Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/src\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Searching for related files ###\n",
      "***internal/clients/hub.go***\n",
      "Why: It manages all the clients and handles the incoming messages.\n",
      "\n",
      "***internal/clients/client.go***\n",
      "Why: It represents each individual client and might have issues while sending messages to the frontend.\n",
      "\n",
      "***internal/fetcher/message.go***\n",
      "Why: It processes the raw messages from the Twitter API and might have issues while forwarding them.\n",
      "\n",
      "***internal/fetcher/fetcher.go***\n",
      "Why: It fetches the raw messages from the Twitter API and might not be sending them to the next processing stage.\n",
      "\n",
      "***internal/router/router.go***\n",
      "Why: It handles the routing of messages between different components and might be causing issues in message delivery.\n",
      "\n",
      "***web/src/atoms/wsAtoms.ts***\n",
      "Why: It manages the websocket connections and state in the frontend and might not be receiving messages.\n",
      "\n",
      "***web/src/utils/messages.ts***\n",
      "Why: It contains utility functions for handling messages and might have issues while processing incoming messages.\n",
      "\n",
      "***web/src/components/TweetBoard/index.tsx***\n",
      "Why: It displays the incoming messages on the frontend and might not be updating the UI with new messages.\n",
      "\n",
      "***web/src/services/worker.type.ts***\n",
      "Why: It defines the types for the service worker and might have issues with the message handling types.\n",
      "\n",
      "***web/src/services/worker.ts***\n",
      "Why: It handles the incoming messages from the backend and might not be forwarding them to the frontend components.\n",
      "### Analysis code in related file ###\n",
      "***\n",
      "internal/clients/hub.go\n",
      "The issue might be related to the Hub not broadcasting messages to the clients properly. Make sure the `broadcastMessage` function is sending messages to the clients correctly. Also, ensure that the `registerClient` and `unregisterClient` functions are working as intended.\n",
      "***\n",
      "\n",
      "***\n",
      "internal/clients/client.go\n",
      "The issue could be related to the `readPump` and `writePump` functions in this file. Make sure that the client is receiving and sending messages correctly through these functions. Also, check if the `updateRaidConfig` function is working properly.\n",
      "***\n",
      "\n",
      "***\n",
      "internal/fetcher/message.go\n",
      "Ensure that the `newRaidMsg` function is creating RaidMsg objects correctly from the raw text. Also, make sure that the `newMessageHandler` and `loadRaidData` functions are working as intended.\n",
      "***\n",
      "\n",
      "***\n",
      "internal/fetcher/fetcher.go\n",
      "Check if the `TweetStreamHandler` function is handling the tweet stream correctly and dispatching the raid messages to the `raidChan`. Also, ensure that the `handleTweet` function is processing the tweets and creating RaidMsg objects properly.\n",
      "***\n",
      "\n",
      "***\n",
      "internal/router/router.go\n",
      "Make sure that the WebSocket endpoints are set up correctly and the communication between the client and the server is working as expected.\n",
      "***\n",
      "\n",
      "***\n",
      "web/src/atoms/wsAtoms.ts\n",
      "Check if the state management related to boards, messages, and raid configuration is working properly. Ensure that the state updates are being propagated correctly to the components.\n",
      "***\n",
      "\n",
      "***\n",
      "web/src/components/TweetBoard/index.tsx\n",
      "Ensure that the components are rendering and updating correctly based on the state changes in the application. Check if the messages are being displayed as expected.\n",
      "***\n",
      "\n",
      "***\n",
      "web/src/services/worker.type.ts & web/src/services/worker.ts\n",
      "Make sure that the WebSocket communication in the worker is working as expected. Check if the messages are being sent and received correctly, and if the worker is processing the messages properly.\n",
      "\n",
      "Possible fix:\n",
      "1. Debug the Hub functions, especially `broadcastMessage`, to see if messages are being sent to the clients correctly.\n",
      "2. Check the Client functions, mainly `readPump` and `writePump`, to see if the clients are receiving and sending messages correctly.\n",
      "3. Ensure that the state management in the front-end is working properly and updates are being reflected in the components.\n",
      "4. Verify that the worker is processing and handling the WebSocket communication correctly.\n",
      "5. Apply necessary fixes based on the findings from the debugging steps above, and test the application again to see if the issue is resolved.\n",
      "### Writing first draft of the pull request ###\n",
      "***\n",
      "{\n",
      "    \"title\": \"Fix: Twitter messages not pushed to frontend\",\n",
      "    \"description\": \"### Why\\nNo twitter messages were being pushed to the frontend, causing the user to not see any new raid messages in the websocket. The issue was traced to multiple parts of the codebase.\\n\\n### What\\nThis pull request fixes the issue by ensuring proper broadcasting of messages, correct handling of tweet streams, and appropriate state management in the frontend. The following changes were made:\\n\\n- Ensure `broadcastMessage`, `registerClient`, and `unregisterClient` functions in `internal/clients/hub.go` work as intended.\\n- Verify `readPump`, `writePump`, and `updateRaidConfig` functions in `internal/clients/client.go` function properly.\\n- Ensure the `newRaidMsg`, `newMessageHandler`, and `loadRaidData` functions in `internal/fetcher/message.go` work as intended.\\n- Check `TweetStreamHandler` and `handleTweet` functions in `internal/fetcher/fetcher.go` for correct handling of tweet streams and RaidMsg objects.\\n- Verify WebSocket endpoints in `internal/router/router.go` are set up correctly.\\n- Ensure proper state management in `web/src/atoms/wsAtoms.ts` related to boards, messages, and raid configuration.\\n- Verify correct rendering and updating of components in `web/src/components/TweetBoard/index.tsx` based on state changes.\",\n",
      "    \"commits\": [\n",
      "        {\n",
      "            \"commit_message\": \"Fix hub functions in internal/clients/hub.go\",\n",
      "            \"commit_files\": [\"internal/clients/hub.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix client and message handling functions\",\n",
      "            \"commit_files\": [\"internal/clients/client.go\", \"internal/fetcher/message.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix tweet stream handling and WebSocket endpoints\",\n",
      "            \"commit_files\": [\"internal/fetcher/fetcher.go\", \"internal/router/router.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix frontend state management and component rendering\",\n",
      "            \"commit_files\": [\"web/src/atoms/wsAtoms.ts\", \"web/src/components/TweetBoard/index.tsx\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "***\n",
      "{'title': 'Fix: Twitter messages not pushed to frontend', 'description': '### Why\\nNo twitter messages were being pushed to the frontend, causing the user to not see any new raid messages in the websocket. The issue was traced to multiple parts of the codebase.\\n\\n### What\\nThis pull request fixes the issue by ensuring proper broadcasting of messages, correct handling of tweet streams, and appropriate state management in the frontend. The following changes were made:\\n\\n- Ensure `broadcastMessage`, `registerClient`, and `unregisterClient` functions in `internal/clients/hub.go` work as intended.\\n- Verify `readPump`, `writePump`, and `updateRaidConfig` functions in `internal/clients/client.go` function properly.\\n- Ensure the `newRaidMsg`, `newMessageHandler`, and `loadRaidData` functions in `internal/fetcher/message.go` work as intended.\\n- Check `TweetStreamHandler` and `handleTweet` functions in `internal/fetcher/fetcher.go` for correct handling of tweet streams and RaidMsg objects.\\n- Verify WebSocket endpoints in `internal/router/router.go` are set up correctly.\\n- Ensure proper state management in `web/src/atoms/wsAtoms.ts` related to boards, messages, and raid configuration.\\n- Verify correct rendering and updating of components in `web/src/components/TweetBoard/index.tsx` based on state changes.', 'commits': [{'commit_message': 'Fix hub functions in internal/clients/hub.go', 'commit_files': ['internal/clients/hub.go']}, {'commit_message': 'Fix client and message handling functions', 'commit_files': ['internal/clients/client.go', 'internal/fetcher/message.go']}, {'commit_message': 'Fix tweet stream handling and WebSocket endpoints', 'commit_files': ['internal/fetcher/fetcher.go', 'internal/router/router.go']}, {'commit_message': 'Fix frontend state management and component rendering', 'commit_files': ['web/src/atoms/wsAtoms.ts', 'web/src/components/TweetBoard/index.tsx']}]}\n"
     ]
    }
   ],
   "source": [
    "gbf_pr = run_chain(\n",
    "    issue_path=Path(\n",
    "        \"/Users/leung/Workspace/personal/sherlock/gbf-raid-finder/issues/test.md\"\n",
    "    ),\n",
    "    code_path=Path(\"/Users/leung/Workspace/personal/sherlock/gbf-raid-finder\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related_file': ['main.py'],\n",
       " 'notes': [\"\\nmain.py\\nThe problem in main.py is in the line where the next node is added to the priority queue: `node_queue.put((-min_node.next.val, counter, min_node.next))`. It's using the negative value of the node which is causing the incorrect order of the output.\\n\\nTo fix the problem, simply change the line to: `node_queue.put((min_node.next.val, counter, min_node.next))`\\n\"],\n",
       " 'suggested_pr': {'title': 'Fix incorrect output order in main.py',\n",
       "  'description': '### Why\\n\\nMain.py do not generate correct output when I run `python main.py`\\n\\n### What\\n\\nFix the issue by changing the line adding the next node to the priority queue to use the positive value of the node, ensuring the correct order of the output.',\n",
       "  'commits': [{'commit_message': 'Fix incorrect output order in main.py',\n",
       "    'commit_files': ['main.py']}]}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_COMMIT_TEMPLATE = \"\"\"\n",
    "We received issue:\n",
    "{issue}\n",
    "\n",
    "And we examined the repository and wrote the following notes:\n",
    "{notes}\n",
    "\n",
    "Here is the code we may want to changes\n",
    "\n",
    "{code_path}\n",
    "{code}\n",
    "\n",
    "Now we want you to generate a diff for this file in the following format:\n",
    "\n",
    "***\n",
    "```diff\n",
    "code diff\n",
    "```\n",
    "***\n",
    "\n",
    "If you think it is not required to change the code, please write `no change` in the diff.\n",
    "Please make sure all changes are included in the diff. Please limit your response to a maximum of {token_limit} tokens.\n",
    "\"\"\"\n",
    "plan_code_prompt = PromptTemplate(\n",
    "    input_variables=[\"issue\", \"notes\", \"code_path\", \"code\", \"token_limit\"],\n",
    "    template=PLAN_COMMIT_TEMPLATE,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_diff(issue, notes, code_path):\n",
    "    human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=plan_code_prompt,\n",
    "    )\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "    code = read_text_by_path(code_path)\n",
    "    ans = chain.run(\n",
    "        issue=issue,\n",
    "        notes=\"\\n\".join(notes),\n",
    "        code_path=code_path.as_posix(),\n",
    "        code=code,\n",
    "        token_limit=TOKEN_LIMIT,\n",
    "    )\n",
    "    print(ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codegen_from_pr(issue_path, analysis, code_path):\n",
    "    suggested_pr = analysis[\"suggested_pr\"]\n",
    "    issue = read_text_by_path(issue_path)\n",
    "    commits = []\n",
    "    notes = analysis[\"notes\"]\n",
    "    for commit in suggested_pr[\"commits\"]:\n",
    "        changes = []\n",
    "        for f in commit[\"commit_files\"]:\n",
    "            change = generate_diff(\n",
    "                issue=issue,\n",
    "                notes=notes,\n",
    "                code_path=code_path / f,\n",
    "            )\n",
    "            changes.append(change)\n",
    "        commits.append({\"message\": commit[\"commit_message\"], \"changes\": changes})\n",
    "    print(commits)\n",
    "    return commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```diff\n",
      "--- main.py\n",
      "+++ main.py\n",
      "@@ -30,7 +30,7 @@\n",
      "             current = current.next\n",
      "             if min_node.next:\n",
      "-                node_queue.put((-min_node.next.val, counter, min_node.next))\n",
      "+                node_queue.put((min_node.next.val, counter, min_node.next))\n",
      "                 counter += 1\n",
      "\n",
      "         return dummy_node.next\n",
      "```\n",
      "[{'message': 'Fix output by correcting priority queue sorting', 'changes': ['```diff\\n--- main.py\\n+++ main.py\\n@@ -30,7 +30,7 @@\\n             current = current.next\\n             if min_node.next:\\n-                node_queue.put((-min_node.next.val, counter, min_node.next))\\n+                node_queue.put((min_node.next.val, counter, min_node.next))\\n                 counter += 1\\n\\n         return dummy_node.next\\n```']}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'message': 'Fix output by correcting priority queue sorting',\n",
       "  'changes': ['```diff\\n--- main.py\\n+++ main.py\\n@@ -30,7 +30,7 @@\\n             current = current.next\\n             if min_node.next:\\n-                node_queue.put((-min_node.next.val, counter, min_node.next))\\n+                node_queue.put((min_node.next.val, counter, min_node.next))\\n                 counter += 1\\n\\n         return dummy_node.next\\n```']}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue = TEST_ISSUE\n",
    "res = codegen_from_pr(\n",
    "    analysis=test_pr,\n",
    "    issue_path=Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/issues/1.md\"),\n",
    "    code_path=Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/src\"),\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 if the error persists..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fix output by correcting priority queue sorting\n",
      "\n",
      "## Why\n",
      "\n",
      "Main.py did not generate the correct output when running `python main.py` due to an issue with the sorting order in the priority queue.\n",
      "\n",
      "## What\n",
      "\n",
      "The proposed solution is to remove the negative sign before `min_node.next.val` in the line where nodes are put back into the priority queue. This will correctly sort the nodes in the priority queue, resulting in the expected output.\n",
      "\n",
      "### Changes\n",
      "\n",
      "1. Commit: Fix output by correcting priority queue sorting \n",
      "    ```diff\n",
      "    --- main.py\n",
      "    +++ main.py\n",
      "    @@ -30,7 +30,7 @@\n",
      "                 current = current.next\n",
      "                 if min_node.next:\n",
      "    -                node_queue.put((-min_node.next.val, counter, min_node.next))\n",
      "    +                node_queue.put((min_node.next.val, counter, min_node.next))\n",
      "                     counter += 1\n",
      "\n",
      "             return dummy_node.next\n",
      "    ```\n",
      "    Explanation: Removed the negative sign before `min_node.next.val` to correctly sort the nodes in the priority queue and generate the expected output.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Fix output by correcting priority queue sorting\\n\\n## Why\\n\\nMain.py did not generate the correct output when running `python main.py` due to an issue with the sorting order in the priority queue.\\n\\n## What\\n\\nThe proposed solution is to remove the negative sign before `min_node.next.val` in the line where nodes are put back into the priority queue. This will correctly sort the nodes in the priority queue, resulting in the expected output.\\n\\n### Changes\\n\\n1. Commit: Fix output by correcting priority queue sorting \\n    ```diff\\n    --- main.py\\n    +++ main.py\\n    @@ -30,7 +30,7 @@\\n                 current = current.next\\n                 if min_node.next:\\n    -                node_queue.put((-min_node.next.val, counter, min_node.next))\\n    +                node_queue.put((min_node.next.val, counter, min_node.next))\\n                     counter += 1\\n\\n             return dummy_node.next\\n    ```\\n    Explanation: Removed the negative sign before `min_node.next.val` to correctly sort the nodes in the priority queue and generate the expected output.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUMMARISE_TEMPLATE = \"\"\"\n",
    "We received issue:\n",
    "{issue}\n",
    "\n",
    "And we examined the repository and wrote the following notes:\n",
    "{notes}\n",
    "\n",
    "And we prepared the follow pr messages\n",
    "{pr_desc}\n",
    "\n",
    "And we generated the following changes:\n",
    "{changes}\n",
    "\n",
    "Now we need to write the final pull request to present our plan and changes to the user. Please write the final pull request in the following format in markdown. Please limit your response to a maximum of {token_limit} tokens\n",
    "\n",
    "# {pr_title}\n",
    "\n",
    "## Why\n",
    "\n",
    "xxx\n",
    "\n",
    "## What\n",
    "\n",
    "xxx\n",
    "\n",
    "### Changes\n",
    "\n",
    "1. Commit: xxx\n",
    "    ```\n",
    "    diff\n",
    "    ```\n",
    "    Explanation: xxx\n",
    "\n",
    "Please show changes that contains actual code changes. If there is no code changes, please skip the commit.\n",
    "\"\"\"\n",
    "write_pr_prompt = PromptTemplate(\n",
    "    input_variables=[\"issue\", \"notes\", \"pr_desc\", \"changes\", \"token_limit\", \"pr_title\"],\n",
    "    template=SUMMARISE_TEMPLATE,\n",
    ")\n",
    "\n",
    "\n",
    "def changes_to_str(changes):\n",
    "    newline = \"\\n\"  # HACK\n",
    "    changes_str = [\n",
    "        f\"Commit: {c['message']} \\n { newline.join(c['changes'])}\" for c in changes\n",
    "    ]\n",
    "    return \"\\n\".join(changes_str)\n",
    "\n",
    "\n",
    "def generate_final_pr(issues, analysis, commits):\n",
    "    human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=write_pr_prompt,\n",
    "    )\n",
    "    chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "    ans = chain.run(\n",
    "        issue=issue,\n",
    "        notes=analysis[\"notes\"],\n",
    "        pr_desc=analysis[\"suggested_pr\"][\"description\"],\n",
    "        pr_title=analysis[\"suggested_pr\"][\"title\"],\n",
    "        changes=changes_to_str(commits),\n",
    "        token_limit=TOKEN_LIMIT,\n",
    "    )\n",
    "    print(ans)\n",
    "    return ans\n",
    "\n",
    "\n",
    "generate_final_pr(TEST_ISSUE, test_pr, res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Searching for related files ###\n",
      "***main.py***\n",
      "Why: It contains relevant code and is the only file in the repository\n",
      "### Analysis code in related file ###\n",
      "***\n",
      "main.py\n",
      "The issue is caused by the wrong sign in the following line:\n",
      "`node_queue.put((-min_node.next.val, counter, min_node.next))`\n",
      "It should be:\n",
      "`node_queue.put((min_node.next.val, counter, min_node.next))`\n",
      "\n",
      "To fix the issue, change the line as mentioned above, and the output will be correct.\n",
      "***\n",
      "### Writing first draft of the pull request ###\n",
      "***\n",
      "{\n",
      "    \"title\": \"Fix incorrect output in main.py\",\n",
      "    \"description\": \"### Why\\n\\nMain.py do not generate correct output when running `python main.py`. The issue is caused by the wrong sign in a line of code.\\n\\n### What\\n\\nChange the line with the wrong sign to fix the issue, and the output will be correct.\",\n",
      "    \"commits\": [\n",
      "        {\n",
      "            \"commit_message\": \"Fix wrong sign in main.py\",\n",
      "            \"commit_files\": [\"main.py\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "***\n",
      "{'title': 'Fix incorrect output in main.py', 'description': '### Why\\n\\nMain.py do not generate correct output when running `python main.py`. The issue is caused by the wrong sign in a line of code.\\n\\n### What\\n\\nChange the line with the wrong sign to fix the issue, and the output will be correct.', 'commits': [{'commit_message': 'Fix wrong sign in main.py', 'commit_files': ['main.py']}]}\n",
      "{ 'notes': [ '\\n'\n",
      "             'main.py\\n'\n",
      "             'The issue is caused by the wrong sign in the following line:\\n'\n",
      "             '`node_queue.put((-min_node.next.val, counter, min_node.next))`\\n'\n",
      "             'It should be:\\n'\n",
      "             '`node_queue.put((min_node.next.val, counter, min_node.next))`\\n'\n",
      "             '\\n'\n",
      "             'To fix the issue, change the line as mentioned above, and the '\n",
      "             'output will be correct.\\n'],\n",
      "  'related_file': ['main.py'],\n",
      "  'suggested_pr': { 'commits': [ { 'commit_files': ['main.py'],\n",
      "                                   'commit_message': 'Fix wrong sign in '\n",
      "                                                     'main.py'}],\n",
      "                    'description': '### Why\\n'\n",
      "                                   '\\n'\n",
      "                                   'Main.py do not generate correct output '\n",
      "                                   'when running `python main.py`. The issue '\n",
      "                                   'is caused by the wrong sign in a line of '\n",
      "                                   'code.\\n'\n",
      "                                   '\\n'\n",
      "                                   '### What\\n'\n",
      "                                   '\\n'\n",
      "                                   'Change the line with the wrong sign to fix '\n",
      "                                   'the issue, and the output will be correct.',\n",
      "                    'title': 'Fix incorrect output in main.py'}}\n",
      "```diff\n",
      "--- main.py\n",
      "+++ main.py\n",
      "@@ -27,7 +27,7 @@\n",
      "             current.next = min_node\n",
      "             current = current.next\n",
      "             if min_node.next:\n",
      "-                node_queue.put((-min_node.next.val, counter, min_node.next))\n",
      "+                node_queue.put((min_node.next.val, counter, min_node.next))\n",
      "                 counter += 1\n",
      "\n",
      "         return dummy_node.next\n",
      "```\n",
      "[{'message': 'Fix wrong sign in main.py', 'changes': ['```diff\\n--- main.py\\n+++ main.py\\n@@ -27,7 +27,7 @@\\n             current.next = min_node\\n             current = current.next\\n             if min_node.next:\\n-                node_queue.put((-min_node.next.val, counter, min_node.next))\\n+                node_queue.put((min_node.next.val, counter, min_node.next))\\n                 counter += 1\\n\\n         return dummy_node.next\\n```']}]\n",
      "[ { 'changes': [ '```diff\\n'\n",
      "                 '--- main.py\\n'\n",
      "                 '+++ main.py\\n'\n",
      "                 '@@ -27,7 +27,7 @@\\n'\n",
      "                 '             current.next = min_node\\n'\n",
      "                 '             current = current.next\\n'\n",
      "                 '             if min_node.next:\\n'\n",
      "                 '-                node_queue.put((-min_node.next.val, '\n",
      "                 'counter, min_node.next))\\n'\n",
      "                 '+                node_queue.put((min_node.next.val, counter, '\n",
      "                 'min_node.next))\\n'\n",
      "                 '                 counter += 1\\n'\n",
      "                 '\\n'\n",
      "                 '         return dummy_node.next\\n'\n",
      "                 '```'],\n",
      "    'message': 'Fix wrong sign in main.py'}]\n",
      "# Fix incorrect output in main.py\n",
      "\n",
      "## Why\n",
      "\n",
      "Main.py does not generate the correct output when running `python main.py`. The issue is caused by the wrong sign in a line of code.\n",
      "\n",
      "## What\n",
      "\n",
      "Change the line with the wrong sign to fix the issue, and the output will be correct.\n",
      "\n",
      "### Changes\n",
      "\n",
      "1. Commit: Fix wrong sign in main.py\n",
      "    ```diff\n",
      "    --- main.py\n",
      "    +++ main.py\n",
      "    @@ -27,7 +27,7 @@\n",
      "                 current.next = min_node\n",
      "                 current = current.next\n",
      "                 if min_node.next:\n",
      "    -                node_queue.put((-min_node.next.val, counter, min_node.next))\n",
      "    +                node_queue.put((min_node.next.val, counter, min_node.next))\n",
      "                     counter += 1\n",
      "\n",
      "             return dummy_node.next\n",
      "    ```\n",
      "    Explanation: The issue is caused by the wrong sign in the following line:\n",
      "    `node_queue.put((-min_node.next.val, counter, min_node.next))`\n",
      "    It should be:\n",
      "    `node_queue.put((min_node.next.val, counter, min_node.next))`\n",
      "    To fix the issue, change the line as mentioned above, and the output will be correct.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Fix incorrect output in main.py\n",
       "\n",
       "## Why\n",
       "\n",
       "Main.py does not generate the correct output when running `python main.py`. The issue is caused by the wrong sign in a line of code.\n",
       "\n",
       "## What\n",
       "\n",
       "Change the line with the wrong sign to fix the issue, and the output will be correct.\n",
       "\n",
       "### Changes\n",
       "\n",
       "1. Commit: Fix wrong sign in main.py\n",
       "    ```diff\n",
       "    --- main.py\n",
       "    +++ main.py\n",
       "    @@ -27,7 +27,7 @@\n",
       "                 current.next = min_node\n",
       "                 current = current.next\n",
       "                 if min_node.next:\n",
       "    -                node_queue.put((-min_node.next.val, counter, min_node.next))\n",
       "    +                node_queue.put((min_node.next.val, counter, min_node.next))\n",
       "                     counter += 1\n",
       "\n",
       "             return dummy_node.next\n",
       "    ```\n",
       "    Explanation: The issue is caused by the wrong sign in the following line:\n",
       "    `node_queue.put((-min_node.next.val, counter, min_node.next))`\n",
       "    It should be:\n",
       "    `node_queue.put((min_node.next.val, counter, min_node.next))`\n",
       "    To fix the issue, change the line as mentioned above, and the output will be correct."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "issue = Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/issues/1.md\")\n",
    "code = Path(\"/Users/leung/Workspace/personal/sherlock/simple_repo/src\")\n",
    "\n",
    "analysis = run_chain(issue, code)\n",
    "\n",
    "pprint.pprint(object=analysis, indent=2)\n",
    "\n",
    "code = codegen_from_pr(issue, analysis, code)\n",
    "\n",
    "pprint.pprint(object=code, indent=2)\n",
    "\n",
    "\n",
    "final_pr = generate_final_pr(issue, analysis, code)\n",
    "display(Markdown(final_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Searching for related files ###\n",
      "***internal/clients/hub.go***\n",
      "Why: It manages the connection between clients and the app, which might not be sending messages to the frontend.\n",
      "\n",
      "***internal/clients/client.go***\n",
      "Why: It handles individual client connections and could be the cause of the issue if messages are not being sent properly.\n",
      "\n",
      "***internal/fetcher/message.go***\n",
      "Why: It processes the messages from Twitter API and could be causing an issue if messages are not being passed onto the frontend.\n",
      "\n",
      "***internal/fetcher/fetcher.go***\n",
      "Why: It fetches messages from Twitter API, and there might be an issue with how they are being handled.\n",
      "\n",
      "***internal/router/router.go***\n",
      "Why: It handles routing within the application, and there could be an issue with the websocket route.\n",
      "\n",
      "***web/src/atoms/wsAtoms.ts***\n",
      "Why: It manages the websocket state for the frontend, and there might be an issue with how messages are being processed.\n",
      "\n",
      "***web/src/utils/messages.ts***\n",
      "Why: It contains utility functions to handle messages, which could be causing issues if they are not working properly.\n",
      "\n",
      "***web/src/components/TweetBoard/index.tsx***\n",
      "Why: It is the main component for displaying the messages, and there might be an issue with how it is listening for new messages.\n",
      "### Analysis code in related file ###\n",
      "***\n",
      "internal/clients/hub.go\n",
      "The issue might be related to the handling of websocket connections and broadcasting messages to the clients. There might be a problem with the registration and unregistration of clients, or with the broadcasting of messages to the clients.\n",
      "\n",
      "Possible solution:\n",
      "1. Check if the clients are being registered and unregistered correctly.\n",
      "2. Verify if the message broadcasting is working as expected, and if the messages are being sent to the appropriate clients.\n",
      "3. Check if there are any issues with the Twitter live message stream and its handling.\n",
      "***\n",
      "\n",
      "***\n",
      "internal/clients/client.go\n",
      "This file handles the middleman between the websocket connection and the hub. There might be issues with reading and writing messages from the websocket connection to the hub.\n",
      "\n",
      "Possible solution:\n",
      "1. Check if the read and write pumps are working correctly.\n",
      "2. Verify if the messages are being handled and processed correctly.\n",
      "3. Investigate any potential issues with the websocket connection and its handling.\n",
      "***\n",
      "\n",
      "***\n",
      "internal/fetcher/message.go\n",
      "This file handles the transformation of tweet messages. There might be issues with the creation of raid messages from tweets or with the message handler.\n",
      "\n",
      "Possible solution:\n",
      "1. Check if the newRaidMsg function is working as expected, and if it's correctly transforming the tweet messages.\n",
      "2. Verify if the message handler is working correctly and handling messages as intended.\n",
      "***\n",
      "\n",
      "***\n",
      "internal/fetcher/fetcher.go\n",
      "This file handles the connection with the Twitter API and the handling of tweet messages. There might be issues with the tweet stream, the handling of tweet messages, or the Twitter API connection.\n",
      "\n",
      "Possible solution:\n",
      "1. Verify if the MakeTweetStream function is working correctly and creating the uplink with the Twitter API.\n",
      "2. Check if the handling of tweet messages in the TweetStreamHandler function is working as expected.\n",
      "3. Investigate any potential issues with the Twitter API connection and its handling.\n",
      "***\n",
      "\n",
      "***\n",
      "web/src/atoms/wsAtoms.ts\n",
      "This file handles the websocket atoms for the application. There might be issues with the management of boards, messages, and their respective atoms.\n",
      "\n",
      "Possible solution:\n",
      "1. Check if the board and message atoms are being managed correctly.\n",
      "2. Verify if the functions for updating and handling the atoms are working as intended.\n",
      "3. Investigate any potential issues with the implementation of the atoms.\n",
      "***\n",
      "\n",
      "***\n",
      "web/src/components/TweetBoard/index.tsx\n",
      "This file handles the rendering of the tweet boards and their respective components. There might be issues with the rendering of the tweet boards, the handling of the board states, or the communication with the websocket atoms.\n",
      "\n",
      "Possible solution:\n",
      "1. Check if the tweet boards are being rendered correctly and handling their respective states as expected.\n",
      "2. Verify if the communication with the websocket atoms is working correctly.\n",
      "3. Investigate any potential issues with the implementation of the tweet board components.\n",
      "***\n",
      "### Writing first draft of the pull request ###\n",
      "***\n",
      "{\n",
      "    \"title\": \"Fix WebSocket issue for Twitter messages\",\n",
      "    \"description\": \"## Why\\n\\nNo twitter message pushed to frontend. The issue might be related to the handling of websocket connections, message broadcasting, and Twitter API connection.\\n\\n## What\\n\\nThis pull request proposes a solution to address the issue by checking and fixing the handling of websocket connections, message broadcasting, and Twitter API connection in the internal/clients, internal/fetcher, and web/src components.\",\n",
      "    \"commits\": [\n",
      "        {\n",
      "            \"commit_message\": \"Fix registration and unregistration of clients in hub.go\",\n",
      "            \"commit_files\": [\"internal/clients/hub.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix read and write pumps in client.go\",\n",
      "            \"commit_files\": [\"internal/clients/client.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix tweet message transformation in message.go\",\n",
      "            \"commit_files\": [\"internal/fetcher/message.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix Twitter API connection and tweet stream handling in fetcher.go\",\n",
      "            \"commit_files\": [\"internal/fetcher/fetcher.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix websocket atoms management in wsAtoms.ts\",\n",
      "            \"commit_files\": [\"web/src/atoms/wsAtoms.ts\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Fix tweet board rendering and communication with websocket atoms in index.tsx\",\n",
      "            \"commit_files\": [\"web/src/components/TweetBoard/index.tsx\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "***\n",
      "{'title': 'Fix WebSocket issue for Twitter messages', 'description': '## Why\\n\\nNo twitter message pushed to frontend. The issue might be related to the handling of websocket connections, message broadcasting, and Twitter API connection.\\n\\n## What\\n\\nThis pull request proposes a solution to address the issue by checking and fixing the handling of websocket connections, message broadcasting, and Twitter API connection in the internal/clients, internal/fetcher, and web/src components.', 'commits': [{'commit_message': 'Fix registration and unregistration of clients in hub.go', 'commit_files': ['internal/clients/hub.go']}, {'commit_message': 'Fix read and write pumps in client.go', 'commit_files': ['internal/clients/client.go']}, {'commit_message': 'Fix tweet message transformation in message.go', 'commit_files': ['internal/fetcher/message.go']}, {'commit_message': 'Fix Twitter API connection and tweet stream handling in fetcher.go', 'commit_files': ['internal/fetcher/fetcher.go']}, {'commit_message': 'Fix websocket atoms management in wsAtoms.ts', 'commit_files': ['web/src/atoms/wsAtoms.ts']}, {'commit_message': 'Fix tweet board rendering and communication with websocket atoms in index.tsx', 'commit_files': ['web/src/components/TweetBoard/index.tsx']}]}\n",
      "{ 'notes': [ '\\n'\n",
      "             'internal/clients/hub.go\\n'\n",
      "             'The issue might be related to the handling of websocket '\n",
      "             'connections and broadcasting messages to the clients. There '\n",
      "             'might be a problem with the registration and unregistration of '\n",
      "             'clients, or with the broadcasting of messages to the clients.\\n'\n",
      "             '\\n'\n",
      "             'Possible solution:\\n'\n",
      "             '1. Check if the clients are being registered and unregistered '\n",
      "             'correctly.\\n'\n",
      "             '2. Verify if the message broadcasting is working as expected, '\n",
      "             'and if the messages are being sent to the appropriate clients.\\n'\n",
      "             '3. Check if there are any issues with the Twitter live message '\n",
      "             'stream and its handling.\\n',\n",
      "             '\\n'\n",
      "             'internal/clients/client.go\\n'\n",
      "             'This file handles the middleman between the websocket connection '\n",
      "             'and the hub. There might be issues with reading and writing '\n",
      "             'messages from the websocket connection to the hub.\\n'\n",
      "             '\\n'\n",
      "             'Possible solution:\\n'\n",
      "             '1. Check if the read and write pumps are working correctly.\\n'\n",
      "             '2. Verify if the messages are being handled and processed '\n",
      "             'correctly.\\n'\n",
      "             '3. Investigate any potential issues with the websocket '\n",
      "             'connection and its handling.\\n',\n",
      "             '\\n'\n",
      "             'internal/fetcher/message.go\\n'\n",
      "             'This file handles the transformation of tweet messages. There '\n",
      "             'might be issues with the creation of raid messages from tweets '\n",
      "             'or with the message handler.\\n'\n",
      "             '\\n'\n",
      "             'Possible solution:\\n'\n",
      "             '1. Check if the newRaidMsg function is working as expected, and '\n",
      "             \"if it's correctly transforming the tweet messages.\\n\"\n",
      "             '2. Verify if the message handler is working correctly and '\n",
      "             'handling messages as intended.\\n',\n",
      "             '\\n'\n",
      "             'internal/fetcher/fetcher.go\\n'\n",
      "             'This file handles the connection with the Twitter API and the '\n",
      "             'handling of tweet messages. There might be issues with the tweet '\n",
      "             'stream, the handling of tweet messages, or the Twitter API '\n",
      "             'connection.\\n'\n",
      "             '\\n'\n",
      "             'Possible solution:\\n'\n",
      "             '1. Verify if the MakeTweetStream function is working correctly '\n",
      "             'and creating the uplink with the Twitter API.\\n'\n",
      "             '2. Check if the handling of tweet messages in the '\n",
      "             'TweetStreamHandler function is working as expected.\\n'\n",
      "             '3. Investigate any potential issues with the Twitter API '\n",
      "             'connection and its handling.\\n',\n",
      "             '\\n'\n",
      "             'web/src/atoms/wsAtoms.ts\\n'\n",
      "             'This file handles the websocket atoms for the application. There '\n",
      "             'might be issues with the management of boards, messages, and '\n",
      "             'their respective atoms.\\n'\n",
      "             '\\n'\n",
      "             'Possible solution:\\n'\n",
      "             '1. Check if the board and message atoms are being managed '\n",
      "             'correctly.\\n'\n",
      "             '2. Verify if the functions for updating and handling the atoms '\n",
      "             'are working as intended.\\n'\n",
      "             '3. Investigate any potential issues with the implementation of '\n",
      "             'the atoms.\\n',\n",
      "             '\\n'\n",
      "             'web/src/components/TweetBoard/index.tsx\\n'\n",
      "             'This file handles the rendering of the tweet boards and their '\n",
      "             'respective components. There might be issues with the rendering '\n",
      "             'of the tweet boards, the handling of the board states, or the '\n",
      "             'communication with the websocket atoms.\\n'\n",
      "             '\\n'\n",
      "             'Possible solution:\\n'\n",
      "             '1. Check if the tweet boards are being rendered correctly and '\n",
      "             'handling their respective states as expected.\\n'\n",
      "             '2. Verify if the communication with the websocket atoms is '\n",
      "             'working correctly.\\n'\n",
      "             '3. Investigate any potential issues with the implementation of '\n",
      "             'the tweet board components.\\n'],\n",
      "  'related_file': [ 'internal/clients/hub.go',\n",
      "                    'internal/clients/client.go',\n",
      "                    'internal/fetcher/message.go',\n",
      "                    'internal/fetcher/fetcher.go',\n",
      "                    'internal/router/router.go',\n",
      "                    'web/src/atoms/wsAtoms.ts',\n",
      "                    'web/src/utils/messages.ts',\n",
      "                    'web/src/components/TweetBoard/index.tsx'],\n",
      "  'suggested_pr': { 'commits': [ { 'commit_files': ['internal/clients/hub.go'],\n",
      "                                   'commit_message': 'Fix registration and '\n",
      "                                                     'unregistration of '\n",
      "                                                     'clients in hub.go'},\n",
      "                                 { 'commit_files': [ 'internal/clients/client.go'],\n",
      "                                   'commit_message': 'Fix read and write pumps '\n",
      "                                                     'in client.go'},\n",
      "                                 { 'commit_files': [ 'internal/fetcher/message.go'],\n",
      "                                   'commit_message': 'Fix tweet message '\n",
      "                                                     'transformation in '\n",
      "                                                     'message.go'},\n",
      "                                 { 'commit_files': [ 'internal/fetcher/fetcher.go'],\n",
      "                                   'commit_message': 'Fix Twitter API '\n",
      "                                                     'connection and tweet '\n",
      "                                                     'stream handling in '\n",
      "                                                     'fetcher.go'},\n",
      "                                 { 'commit_files': ['web/src/atoms/wsAtoms.ts'],\n",
      "                                   'commit_message': 'Fix websocket atoms '\n",
      "                                                     'management in '\n",
      "                                                     'wsAtoms.ts'},\n",
      "                                 { 'commit_files': [ 'web/src/components/TweetBoard/index.tsx'],\n",
      "                                   'commit_message': 'Fix tweet board '\n",
      "                                                     'rendering and '\n",
      "                                                     'communication with '\n",
      "                                                     'websocket atoms in '\n",
      "                                                     'index.tsx'}],\n",
      "                    'description': '## Why\\n'\n",
      "                                   '\\n'\n",
      "                                   'No twitter message pushed to frontend. The '\n",
      "                                   'issue might be related to the handling of '\n",
      "                                   'websocket connections, message '\n",
      "                                   'broadcasting, and Twitter API connection.\\n'\n",
      "                                   '\\n'\n",
      "                                   '## What\\n'\n",
      "                                   '\\n'\n",
      "                                   'This pull request proposes a solution to '\n",
      "                                   'address the issue by checking and fixing '\n",
      "                                   'the handling of websocket connections, '\n",
      "                                   'message broadcasting, and Twitter API '\n",
      "                                   'connection in the internal/clients, '\n",
      "                                   'internal/fetcher, and web/src components.',\n",
      "                    'title': 'Fix WebSocket issue for Twitter messages'}}\n",
      "```diff\n",
      "no change\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 if the error persists..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 if the error persists..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 6 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 2 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions. Please follow the possible solutions listed above to identify and resolve the issue.\n",
      "```diff\n",
      "no change\n",
      "```\n",
      "```diff\n",
      "no change\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 4 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 3 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 3 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 2 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change\n",
      "[{'message': 'Fix registration and unregistration of clients in hub.go', 'changes': ['```diff\\nno change\\n```']}, {'message': 'Fix read and write pumps in client.go', 'changes': ['There is no specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions. Please follow the possible solutions listed above to identify and resolve the issue.']}, {'message': 'Fix tweet message transformation in message.go', 'changes': ['```diff\\nno change\\n```']}, {'message': 'Fix Twitter API connection and tweet stream handling in fetcher.go', 'changes': ['```diff\\nno change\\n```']}, {'message': 'Fix websocket atoms management in wsAtoms.ts', 'changes': ['no change']}, {'message': 'Fix tweet board rendering and communication with websocket atoms in index.tsx', 'changes': ['no change']}]\n",
      "[ { 'changes': ['```diff\\nno change\\n```'],\n",
      "    'message': 'Fix registration and unregistration of clients in hub.go'},\n",
      "  { 'changes': [ 'There is no specific code change to suggest at this moment, '\n",
      "                 'as the issue requires further investigation and debugging in '\n",
      "                 'the mentioned files and functions. Please follow the '\n",
      "                 'possible solutions listed above to identify and resolve the '\n",
      "                 'issue.'],\n",
      "    'message': 'Fix read and write pumps in client.go'},\n",
      "  { 'changes': ['```diff\\nno change\\n```'],\n",
      "    'message': 'Fix tweet message transformation in message.go'},\n",
      "  { 'changes': ['```diff\\nno change\\n```'],\n",
      "    'message': 'Fix Twitter API connection and tweet stream handling in '\n",
      "               'fetcher.go'},\n",
      "  { 'changes': ['no change'],\n",
      "    'message': 'Fix websocket atoms management in wsAtoms.ts'},\n",
      "  { 'changes': ['no change'],\n",
      "    'message': 'Fix tweet board rendering and communication with websocket '\n",
      "               'atoms in index.tsx'}]\n",
      "# Fix WebSocket issue for Twitter messages\n",
      "\n",
      "## Why\n",
      "\n",
      "No twitter message pushed to frontend. The issue might be related to the handling of websocket connections, message broadcasting, and Twitter API connection.\n",
      "\n",
      "## What\n",
      "\n",
      "This pull request proposes a solution to address the issue by checking and fixing the handling of websocket connections, message broadcasting, and Twitter API connection in the internal/clients, internal/fetcher, and web/src components.\n",
      "\n",
      "### Changes\n",
      "\n",
      "1. Fix registration and unregistration of clients in hub.go\n",
      "\n",
      "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions. \n",
      "\n",
      "2. Fix read and write pumps in client.go\n",
      "\n",
      "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
      "\n",
      "3. Fix tweet message transformation in message.go\n",
      "\n",
      "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
      "\n",
      "4. Fix Twitter API connection and tweet stream handling in fetcher.go\n",
      "\n",
      "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
      "\n",
      "5. Fix websocket atoms management in wsAtoms.ts\n",
      "\n",
      "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
      "\n",
      "6. Fix tweet board rendering and communication with websocket atoms in index.tsx\n",
      "\n",
      "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
      "\n",
      "Please follow the possible solutions listed above to identify and resolve the issue.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Fix WebSocket issue for Twitter messages\n",
       "\n",
       "## Why\n",
       "\n",
       "No twitter message pushed to frontend. The issue might be related to the handling of websocket connections, message broadcasting, and Twitter API connection.\n",
       "\n",
       "## What\n",
       "\n",
       "This pull request proposes a solution to address the issue by checking and fixing the handling of websocket connections, message broadcasting, and Twitter API connection in the internal/clients, internal/fetcher, and web/src components.\n",
       "\n",
       "### Changes\n",
       "\n",
       "1. Fix registration and unregistration of clients in hub.go\n",
       "\n",
       "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions. \n",
       "\n",
       "2. Fix read and write pumps in client.go\n",
       "\n",
       "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
       "\n",
       "3. Fix tweet message transformation in message.go\n",
       "\n",
       "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
       "\n",
       "4. Fix Twitter API connection and tweet stream handling in fetcher.go\n",
       "\n",
       "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
       "\n",
       "5. Fix websocket atoms management in wsAtoms.ts\n",
       "\n",
       "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
       "\n",
       "6. Fix tweet board rendering and communication with websocket atoms in index.tsx\n",
       "\n",
       "    No specific code change to suggest at this moment, as the issue requires further investigation and debugging in the mentioned files and functions.\n",
       "\n",
       "Please follow the possible solutions listed above to identify and resolve the issue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issue = Path(\"/Users/leung/Workspace/personal/sherlock/gbf-raid-finder/issues/test.md\")\n",
    "code = Path(\"/Users/leung/Workspace/personal/sherlock/gbf-raid-finder\")\n",
    "\n",
    "analysis = run_chain(issue, code)\n",
    "\n",
    "pprint.pprint(object=analysis, indent=2)\n",
    "\n",
    "code = codegen_from_pr(issue, analysis, code)\n",
    "\n",
    "pprint.pprint(object=code, indent=2)\n",
    "\n",
    "\n",
    "final_pr = generate_final_pr(issue, analysis, code)\n",
    "display(Markdown(final_pr))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Searching for related files ###\n",
      "***options.go***\n",
      "Why: It contains the code for handling options, which is where the WithConcurrency option should be added.\n",
      "\n",
      "***engine.go***\n",
      "Why: It contains the main engine code, which is where the concurrency logic should be implemented.\n",
      "\n",
      "***engine_test.go***\n",
      "Why: It contains tests for the engine, which is where new tests for the WithConcurrency option should be added.\n",
      "### Analysis code in related file ###\n",
      "***\n",
      "options.go\n",
      "- Add a new option function WithConcurrency to set the concurrency level (number of goroutines)\n",
      "***\n",
      "```go\n",
      "// WithConcurrency returns an Option that sets the concurrency level for consuming messages.\n",
      "func WithConcurrency(concurrency int) Option {\n",
      "\treturn func(c *Config) {\n",
      "\t\tif concurrency > 0 {\n",
      "\t\t\tc.Concurrency = concurrency\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "```\n",
      "- Update the Config struct to include a Concurrency field\n",
      "***\n",
      "```go\n",
      "type Config struct {\n",
      "\t// ...\n",
      "\tConcurrency            int\n",
      "\t// ...\n",
      "}\n",
      "```\n",
      "- Update the newDefaultConfig function to set the default concurrency level to 1\n",
      "***\n",
      "```go\n",
      "func newDefaultConfig() *Config {\n",
      "\treturn &Config{\n",
      "\t\t// ...\n",
      "\t\tConcurrency:            1,\n",
      "\t\t// ...\n",
      "\t}\n",
      "}\n",
      "```\n",
      "\n",
      "***\n",
      "engine.go\n",
      "- Update the newEngine function to pass the concurrency level to the process creation\n",
      "***\n",
      "```go\n",
      "func newEngine(subscriber Subscriber, bConsumer BatchConsumer, consumer Consumer, opts ...Option) *Engine {\n",
      "\tcfg := newDefaultConfig()\n",
      "\t// ...\n",
      "\te := &Engine{\n",
      "\t\t// ...\n",
      "\t}\n",
      "\n",
      "\treturn e\n",
      "}\n",
      "```\n",
      "- Update the Start method of the Engine struct to create multiple goroutines for consuming messages based on the concurrency level\n",
      "***\n",
      "```go\n",
      "func (e *Engine) Start(ctx context.Context) error {\n",
      "\t// ...\n",
      "\tctx = setLogger(ctx, e.Logger)\n",
      "\n",
      "\tvar wg sync.WaitGroup\n",
      "\tfor i := 0; i < e.Config.Concurrency; i++ {\n",
      "\t\twg.Add(1)\n",
      "\t\tgo func() {\n",
      "\t\t\tdefer wg.Done()\n",
      "\t\t\terr := errors.WithStack(newProcess(e).Start(ctx))\n",
      "\t\t\tif err != nil {\n",
      "\t\t\t\te.Logger.Printf(\"Error in the process: %v\", err)\n",
      "\t\t\t}\n",
      "\t\t}()\n",
      "\t}\n",
      "\n",
      "\twg.Wait()\n",
      "\treturn nil\n",
      "}\n",
      "```\n",
      "\n",
      "***\n",
      "engine_test.go\n",
      "- Add test cases for the WithConcurrency option to verify that the correct number of goroutines are created and messages are consumed concurrently\n",
      "***\n",
      "```go\n",
      "func TestEngineWithConcurrency(t *testing.T) {\n",
      "\t// ...\n",
      "}\n",
      "```\n",
      "### Writing first draft of the pull request ###\n",
      "{\n",
      "    \"title\": \"Add WithConcurrency option to control the number of goroutines\",\n",
      "    \"description\": \"### Why\\n\\nThere was a need to control the number of goroutines that consume messages concurrently. The default behavior was to consume messages using a single goroutine.\\n\\n### What\\n\\nThis pull request adds a new `WithConcurrency` option to set the desired concurrency level (number of goroutines). By using this option, one can control the number of goroutines that consume messages concurrently. The default concurrency level is set to 1.\",\n",
      "    \"commits\": [\n",
      "        {\n",
      "            \"commit_message\": \"Add WithConcurrency option to options.go\",\n",
      "            \"commit_files\": [\"options.go\"]\n",
      "        },\n",
      "        {\n",
      "            \"commit_message\": \"Update engine.go to use the concurrency setting\",\n",
      "            \"commit_files\": [\"engine.go\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{'title': 'Add WithConcurrency option to control the number of goroutines', 'description': '### Why\\n\\nThere was a need to control the number of goroutines that consume messages concurrently. The default behavior was to consume messages using a single goroutine.\\n\\n### What\\n\\nThis pull request adds a new `WithConcurrency` option to set the desired concurrency level (number of goroutines). By using this option, one can control the number of goroutines that consume messages concurrently. The default concurrency level is set to 1.', 'commits': [{'commit_message': 'Add WithConcurrency option to options.go', 'commit_files': ['options.go']}, {'commit_message': 'Update engine.go to use the concurrency setting', 'commit_files': ['engine.go']}]}\n",
      "{ 'notes': [ '\\n'\n",
      "             'options.go\\n'\n",
      "             '- Add a new option function WithConcurrency to set the '\n",
      "             'concurrency level (number of goroutines)\\n',\n",
      "             '\\n'\n",
      "             '```go\\n'\n",
      "             'type Config struct {\\n'\n",
      "             '\\t// ...\\n'\n",
      "             '\\tConcurrency            int\\n'\n",
      "             '\\t// ...\\n'\n",
      "             '}\\n'\n",
      "             '```\\n'\n",
      "             '- Update the newDefaultConfig function to set the default '\n",
      "             'concurrency level to 1\\n',\n",
      "             '\\n'\n",
      "             'engine.go\\n'\n",
      "             '- Update the newEngine function to pass the concurrency level to '\n",
      "             'the process creation\\n',\n",
      "             '\\n'\n",
      "             '```go\\n'\n",
      "             'func (e *Engine) Start(ctx context.Context) error {\\n'\n",
      "             '\\t// ...\\n'\n",
      "             '\\tctx = setLogger(ctx, e.Logger)\\n'\n",
      "             '\\n'\n",
      "             '\\tvar wg sync.WaitGroup\\n'\n",
      "             '\\tfor i := 0; i < e.Config.Concurrency; i++ {\\n'\n",
      "             '\\t\\twg.Add(1)\\n'\n",
      "             '\\t\\tgo func() {\\n'\n",
      "             '\\t\\t\\tdefer wg.Done()\\n'\n",
      "             '\\t\\t\\terr := errors.WithStack(newProcess(e).Start(ctx))\\n'\n",
      "             '\\t\\t\\tif err != nil {\\n'\n",
      "             '\\t\\t\\t\\te.Logger.Printf(\"Error in the process: %v\", err)\\n'\n",
      "             '\\t\\t\\t}\\n'\n",
      "             '\\t\\t}()\\n'\n",
      "             '\\t}\\n'\n",
      "             '\\n'\n",
      "             '\\twg.Wait()\\n'\n",
      "             '\\treturn nil\\n'\n",
      "             '}\\n'\n",
      "             '```\\n'\n",
      "             '\\n'],\n",
      "  'related_file': ['options.go', 'engine.go', 'engine_test.go'],\n",
      "  'suggested_pr': { 'commits': [ { 'commit_files': ['options.go'],\n",
      "                                   'commit_message': 'Add WithConcurrency '\n",
      "                                                     'option to options.go'},\n",
      "                                 { 'commit_files': ['engine.go'],\n",
      "                                   'commit_message': 'Update engine.go to use '\n",
      "                                                     'the concurrency '\n",
      "                                                     'setting'}],\n",
      "                    'description': '### Why\\n'\n",
      "                                   '\\n'\n",
      "                                   'There was a need to control the number of '\n",
      "                                   'goroutines that consume messages '\n",
      "                                   'concurrently. The default behavior was to '\n",
      "                                   'consume messages using a single '\n",
      "                                   'goroutine.\\n'\n",
      "                                   '\\n'\n",
      "                                   '### What\\n'\n",
      "                                   '\\n'\n",
      "                                   'This pull request adds a new '\n",
      "                                   '`WithConcurrency` option to set the '\n",
      "                                   'desired concurrency level (number of '\n",
      "                                   'goroutines). By using this option, one can '\n",
      "                                   'control the number of goroutines that '\n",
      "                                   'consume messages concurrently. The default '\n",
      "                                   'concurrency level is set to 1.',\n",
      "                    'title': 'Add WithConcurrency option to control the number '\n",
      "                             'of goroutines'}}\n",
      "```diff\n",
      "diff --git a/options.go b/options.go\n",
      "index 1234567..abcdefg 100644\n",
      "--- a/options.go\n",
      "+++ b/options.go\n",
      "@@ -14,6 +14,18 @@ func WithConsumerInterceptors(interceptors ...ConsumerInterceptor) Option {\n",
      " \t}\n",
      " }\n",
      " \n",
      "+// WithConcurrency returns an Option that sets the number of goroutines that will consume messages concurrently.\n",
      "+func WithConcurrency(concurrency int) Option {\n",
      "+\treturn func(c *Config) {\n",
      "+\t\tif concurrency > 0 {\n",
      "+\t\t\tc.Concurrency = concurrency\n",
      "+\t\t} else {\n",
      "+\t\t\tc.Concurrency = 1\n",
      "+\t\t}\n",
      "+\t}\n",
      "+}\n",
      "+\n",
      " // WithBatchConsumerInterceptors returns an Option that sets the BatchConsumerInterceptor implementations(s).\n",
      " // Interceptors are called in order of addition.\n",
      " // e.g) interceptor1, interceptor2, interceptor3 => interceptor1 => interceptor2 => interceptor3 => BatchConsumer.Consume\n",
      "```\n",
      "```diff\n",
      "diff --git a/engine.go b/engine.go\n",
      "index 6c42d6b..b6c8fbd 100644\n",
      "--- a/engine.go\n",
      "+++ b/engine.go\n",
      "@@ -21,6 +21,7 @@ func NewBatch(subscriber Subscriber, consumer BatchConsumer, opts ...Option) *En\n",
      " \tcfg := newDefaultConfig()\n",
      " \tcfg.BatchConsumer = bConsumer\n",
      " \tcfg.Consumer = consumer\n",
      "+\tcfg.Concurrency = 1\n",
      " \tcfg.apply(opts)\n",
      " \n",
      " \te := &Engine{\n",
      "@@ -33,12 +34,20 @@ func newEngine(subscriber Subscriber, bConsumer BatchConsumer, consumer Consume\n",
      " // Start starts Subscriber and Consumer process.\n",
      " func (e *Engine) Start(ctx context.Context) error {\n",
      " \te.Logger.Print(\"Start Pub/Sub worker\")\n",
      "-\tdefer e.Logger.Print(\"Finish Pub/Sub worker\")\n",
      "+\tdefer e.Logger.Print(\"Finish all Pub/Sub worker\")\n",
      " \n",
      " \tctx = setLogger(ctx, e.Logger)\n",
      " \n",
      "-\treturn errors.WithStack(newProcess(e).Start(ctx))\n",
      "+\tvar wg sync.WaitGroup\n",
      "+\tfor i := 0; i < e.Config.Concurrency; i++ {\n",
      "+\t\twg.Add(1)\n",
      "+\t\tgo func() {\n",
      "+\t\t\tdefer wg.Done()\n",
      "+\t\t\terr := errors.WithStack(newProcess(e).Start(ctx))\n",
      "+\t\t\tif err != nil {\n",
      "+\t\t\t\te.Logger.Printf(\"Error in the process: %v\", err)\n",
      "+\t\t\t}\n",
      "+\t\t}()\n",
      "+\t}\n",
      "+\twg.Wait()\n",
      "+\treturn nil\n",
      " }\n",
      "```\n",
      "[{'message': 'Add WithConcurrency option to options.go', 'changes': ['```diff\\ndiff --git a/options.go b/options.go\\nindex 1234567..abcdefg 100644\\n--- a/options.go\\n+++ b/options.go\\n@@ -14,6 +14,18 @@ func WithConsumerInterceptors(interceptors ...ConsumerInterceptor) Option {\\n \\t}\\n }\\n \\n+// WithConcurrency returns an Option that sets the number of goroutines that will consume messages concurrently.\\n+func WithConcurrency(concurrency int) Option {\\n+\\treturn func(c *Config) {\\n+\\t\\tif concurrency > 0 {\\n+\\t\\t\\tc.Concurrency = concurrency\\n+\\t\\t} else {\\n+\\t\\t\\tc.Concurrency = 1\\n+\\t\\t}\\n+\\t}\\n+}\\n+\\n // WithBatchConsumerInterceptors returns an Option that sets the BatchConsumerInterceptor implementations(s).\\n // Interceptors are called in order of addition.\\n // e.g) interceptor1, interceptor2, interceptor3 => interceptor1 => interceptor2 => interceptor3 => BatchConsumer.Consume\\n```']}, {'message': 'Update engine.go to use the concurrency setting', 'changes': ['```diff\\ndiff --git a/engine.go b/engine.go\\nindex 6c42d6b..b6c8fbd 100644\\n--- a/engine.go\\n+++ b/engine.go\\n@@ -21,6 +21,7 @@ func NewBatch(subscriber Subscriber, consumer BatchConsumer, opts ...Option) *En\\n \\tcfg := newDefaultConfig()\\n \\tcfg.BatchConsumer = bConsumer\\n \\tcfg.Consumer = consumer\\n+\\tcfg.Concurrency = 1\\n \\tcfg.apply(opts)\\n \\n \\te := &Engine{\\n@@ -33,12 +34,20 @@ func newEngine(subscriber Subscriber, bConsumer BatchConsumer, consumer Consume\\n // Start starts Subscriber and Consumer process.\\n func (e *Engine) Start(ctx context.Context) error {\\n \\te.Logger.Print(\"Start Pub/Sub worker\")\\n-\\tdefer e.Logger.Print(\"Finish Pub/Sub worker\")\\n+\\tdefer e.Logger.Print(\"Finish all Pub/Sub worker\")\\n \\n \\tctx = setLogger(ctx, e.Logger)\\n \\n-\\treturn errors.WithStack(newProcess(e).Start(ctx))\\n+\\tvar wg sync.WaitGroup\\n+\\tfor i := 0; i < e.Config.Concurrency; i++ {\\n+\\t\\twg.Add(1)\\n+\\t\\tgo func() {\\n+\\t\\t\\tdefer wg.Done()\\n+\\t\\t\\terr := errors.WithStack(newProcess(e).Start(ctx))\\n+\\t\\t\\tif err != nil {\\n+\\t\\t\\t\\te.Logger.Printf(\"Error in the process: %v\", err)\\n+\\t\\t\\t}\\n+\\t\\t}()\\n+\\t}\\n+\\twg.Wait()\\n+\\treturn nil\\n }\\n```']}]\n",
      "[ { 'changes': [ '```diff\\n'\n",
      "                 'diff --git a/options.go b/options.go\\n'\n",
      "                 'index 1234567..abcdefg 100644\\n'\n",
      "                 '--- a/options.go\\n'\n",
      "                 '+++ b/options.go\\n'\n",
      "                 '@@ -14,6 +14,18 @@ func '\n",
      "                 'WithConsumerInterceptors(interceptors '\n",
      "                 '...ConsumerInterceptor) Option {\\n'\n",
      "                 ' \\t}\\n'\n",
      "                 ' }\\n'\n",
      "                 ' \\n'\n",
      "                 '+// WithConcurrency returns an Option that sets the number '\n",
      "                 'of goroutines that will consume messages concurrently.\\n'\n",
      "                 '+func WithConcurrency(concurrency int) Option {\\n'\n",
      "                 '+\\treturn func(c *Config) {\\n'\n",
      "                 '+\\t\\tif concurrency > 0 {\\n'\n",
      "                 '+\\t\\t\\tc.Concurrency = concurrency\\n'\n",
      "                 '+\\t\\t} else {\\n'\n",
      "                 '+\\t\\t\\tc.Concurrency = 1\\n'\n",
      "                 '+\\t\\t}\\n'\n",
      "                 '+\\t}\\n'\n",
      "                 '+}\\n'\n",
      "                 '+\\n'\n",
      "                 ' // WithBatchConsumerInterceptors returns an Option that '\n",
      "                 'sets the BatchConsumerInterceptor implementations(s).\\n'\n",
      "                 ' // Interceptors are called in order of addition.\\n'\n",
      "                 ' // e.g) interceptor1, interceptor2, interceptor3 => '\n",
      "                 'interceptor1 => interceptor2 => interceptor3 => '\n",
      "                 'BatchConsumer.Consume\\n'\n",
      "                 '```'],\n",
      "    'message': 'Add WithConcurrency option to options.go'},\n",
      "  { 'changes': [ '```diff\\n'\n",
      "                 'diff --git a/engine.go b/engine.go\\n'\n",
      "                 'index 6c42d6b..b6c8fbd 100644\\n'\n",
      "                 '--- a/engine.go\\n'\n",
      "                 '+++ b/engine.go\\n'\n",
      "                 '@@ -21,6 +21,7 @@ func NewBatch(subscriber Subscriber, '\n",
      "                 'consumer BatchConsumer, opts ...Option) *En\\n'\n",
      "                 ' \\tcfg := newDefaultConfig()\\n'\n",
      "                 ' \\tcfg.BatchConsumer = bConsumer\\n'\n",
      "                 ' \\tcfg.Consumer = consumer\\n'\n",
      "                 '+\\tcfg.Concurrency = 1\\n'\n",
      "                 ' \\tcfg.apply(opts)\\n'\n",
      "                 ' \\n'\n",
      "                 ' \\te := &Engine{\\n'\n",
      "                 '@@ -33,12 +34,20 @@ func newEngine(subscriber Subscriber, '\n",
      "                 'bConsumer BatchConsumer, consumer Consume\\n'\n",
      "                 ' // Start starts Subscriber and Consumer process.\\n'\n",
      "                 ' func (e *Engine) Start(ctx context.Context) error {\\n'\n",
      "                 ' \\te.Logger.Print(\"Start Pub/Sub worker\")\\n'\n",
      "                 '-\\tdefer e.Logger.Print(\"Finish Pub/Sub worker\")\\n'\n",
      "                 '+\\tdefer e.Logger.Print(\"Finish all Pub/Sub worker\")\\n'\n",
      "                 ' \\n'\n",
      "                 ' \\tctx = setLogger(ctx, e.Logger)\\n'\n",
      "                 ' \\n'\n",
      "                 '-\\treturn errors.WithStack(newProcess(e).Start(ctx))\\n'\n",
      "                 '+\\tvar wg sync.WaitGroup\\n'\n",
      "                 '+\\tfor i := 0; i < e.Config.Concurrency; i++ {\\n'\n",
      "                 '+\\t\\twg.Add(1)\\n'\n",
      "                 '+\\t\\tgo func() {\\n'\n",
      "                 '+\\t\\t\\tdefer wg.Done()\\n'\n",
      "                 '+\\t\\t\\terr := errors.WithStack(newProcess(e).Start(ctx))\\n'\n",
      "                 '+\\t\\t\\tif err != nil {\\n'\n",
      "                 '+\\t\\t\\t\\te.Logger.Printf(\"Error in the process: %v\", err)\\n'\n",
      "                 '+\\t\\t\\t}\\n'\n",
      "                 '+\\t\\t}()\\n'\n",
      "                 '+\\t}\\n'\n",
      "                 '+\\twg.Wait()\\n'\n",
      "                 '+\\treturn nil\\n'\n",
      "                 ' }\\n'\n",
      "                 '```'],\n",
      "    'message': 'Update engine.go to use the concurrency setting'}]\n",
      "# Add WithConcurrency option to control the number of goroutines\n",
      "\n",
      "## Why\n",
      "\n",
      "There was a need to control the number of goroutines that consume messages concurrently. The default behavior was to consume messages using a single goroutine.\n",
      "\n",
      "## What\n",
      "\n",
      "This pull request adds a new `WithConcurrency` option to set the desired concurrency level (number of goroutines). By using this option, one can control the number of goroutines that consume messages concurrently. The default concurrency level is set to 1.\n",
      "\n",
      "### Changes\n",
      "\n",
      "1. Commit: Add WithConcurrency option to options.go\n",
      "    ```diff\n",
      "    diff --git a/options.go b/options.go\n",
      "    index 1234567..abcdefg 100644\n",
      "    --- a/options.go\n",
      "    +++ b/options.go\n",
      "    @@ -14,6 +14,18 @@ func WithConsumerInterceptors(interceptors ...ConsumerInterceptor) Option {\n",
      "     }\n",
      "    }\n",
      "\n",
      "    +// WithConcurrency returns an Option that sets the number of goroutines that will consume messages concurrently.\n",
      "    +func WithConcurrency(concurrency int) Option {\n",
      "    +\treturn func(c *Config) {\n",
      "    +\t\tif concurrency > 0 {\n",
      "    +\t\t\tc.Concurrency = concurrency\n",
      "    +\t\t} else {\n",
      "    +\t\t\tc.Concurrency = 1\n",
      "    +\t\t}\n",
      "    +\t}\n",
      "    +}\n",
      "    +\n",
      "    // WithBatchConsumerInterceptors returns an Option that sets the BatchConsumerInterceptor implementations(s).\n",
      "    // Interceptors are called in order of addition.\n",
      "    // e.g) interceptor1, interceptor2, interceptor3 => interceptor1 => interceptor2 => interceptor3 => BatchConsumer.Consume\n",
      "    ```\n",
      "\n",
      "2. Commit: Update engine.go to use the concurrency setting\n",
      "    ```diff\n",
      "    diff --git a/engine.go b/engine.go\n",
      "    index 6c42d6b..b6c8fbd 100644\n",
      "    --- a/engine.go\n",
      "    +++ b/engine.go\n",
      "    @@ -21,6 +21,7 @@ func NewBatch(subscriber Subscriber, consumer BatchConsumer, opts ...Option) *En\n",
      "     cfg := newDefaultConfig()\n",
      "     cfg.BatchConsumer = bConsumer\n",
      "     cfg.Consumer = consumer\n",
      "    +\tcfg.Concurrency = 1\n",
      "     cfg.apply(opts)\n",
      "\n",
      "     e := &Engine{\n",
      "    @@ -33,12 +34,20 @@ func newEngine(subscriber Subscriber, bConsumer BatchConsumer, consumer Consume\n",
      "    // Start starts Subscriber and Consumer process.\n",
      "    func (e *Engine) Start(ctx context.Context) error {\n",
      "     e.Logger.Print(\"Start Pub/Sub worker\")\n",
      "    -\tdefer e.Logger.Print(\"Finish Pub/Sub worker\")\n",
      "    +\tdefer e.Logger.Print(\"Finish all Pub/Sub worker\")\n",
      "\n",
      "     ctx = setLogger(ctx, e.Logger)\n",
      "\n",
      "    -\treturn errors.WithStack(newProcess(e).Start(ctx))\n",
      "    +\tvar wg sync.WaitGroup\n",
      "    +\tfor i := 0; i < e.Config.Concurrency; i++ {\n",
      "    +\t\twg.Add(1)\n",
      "    +\t\tgo func() {\n",
      "    +\t\t\tdefer wg.Done()\n",
      "    +\t\t\terr := errors.WithStack(newProcess(e).Start(ctx))\n",
      "    +\t\t\tif err != nil {\n",
      "    +\t\t\t\te.Logger.Printf(\"Error in the process: %v\", err)\n",
      "    +\t\t\t}\n",
      "    +\t\t}()\n",
      "    +\t}\n",
      "    +\twg.Wait()\n",
      "    +\treturn nil\n",
      "    }\n",
      "    ```\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Add WithConcurrency option to control the number of goroutines\n",
       "\n",
       "## Why\n",
       "\n",
       "There was a need to control the number of goroutines that consume messages concurrently. The default behavior was to consume messages using a single goroutine.\n",
       "\n",
       "## What\n",
       "\n",
       "This pull request adds a new `WithConcurrency` option to set the desired concurrency level (number of goroutines). By using this option, one can control the number of goroutines that consume messages concurrently. The default concurrency level is set to 1.\n",
       "\n",
       "### Changes\n",
       "\n",
       "1. Commit: Add WithConcurrency option to options.go\n",
       "    ```diff\n",
       "    diff --git a/options.go b/options.go\n",
       "    index 1234567..abcdefg 100644\n",
       "    --- a/options.go\n",
       "    +++ b/options.go\n",
       "    @@ -14,6 +14,18 @@ func WithConsumerInterceptors(interceptors ...ConsumerInterceptor) Option {\n",
       "     }\n",
       "    }\n",
       "\n",
       "    +// WithConcurrency returns an Option that sets the number of goroutines that will consume messages concurrently.\n",
       "    +func WithConcurrency(concurrency int) Option {\n",
       "    +\treturn func(c *Config) {\n",
       "    +\t\tif concurrency > 0 {\n",
       "    +\t\t\tc.Concurrency = concurrency\n",
       "    +\t\t} else {\n",
       "    +\t\t\tc.Concurrency = 1\n",
       "    +\t\t}\n",
       "    +\t}\n",
       "    +}\n",
       "    +\n",
       "    // WithBatchConsumerInterceptors returns an Option that sets the BatchConsumerInterceptor implementations(s).\n",
       "    // Interceptors are called in order of addition.\n",
       "    // e.g) interceptor1, interceptor2, interceptor3 => interceptor1 => interceptor2 => interceptor3 => BatchConsumer.Consume\n",
       "    ```\n",
       "\n",
       "2. Commit: Update engine.go to use the concurrency setting\n",
       "    ```diff\n",
       "    diff --git a/engine.go b/engine.go\n",
       "    index 6c42d6b..b6c8fbd 100644\n",
       "    --- a/engine.go\n",
       "    +++ b/engine.go\n",
       "    @@ -21,6 +21,7 @@ func NewBatch(subscriber Subscriber, consumer BatchConsumer, opts ...Option) *En\n",
       "     cfg := newDefaultConfig()\n",
       "     cfg.BatchConsumer = bConsumer\n",
       "     cfg.Consumer = consumer\n",
       "    +\tcfg.Concurrency = 1\n",
       "     cfg.apply(opts)\n",
       "\n",
       "     e := &Engine{\n",
       "    @@ -33,12 +34,20 @@ func newEngine(subscriber Subscriber, bConsumer BatchConsumer, consumer Consume\n",
       "    // Start starts Subscriber and Consumer process.\n",
       "    func (e *Engine) Start(ctx context.Context) error {\n",
       "     e.Logger.Print(\"Start Pub/Sub worker\")\n",
       "    -\tdefer e.Logger.Print(\"Finish Pub/Sub worker\")\n",
       "    +\tdefer e.Logger.Print(\"Finish all Pub/Sub worker\")\n",
       "\n",
       "     ctx = setLogger(ctx, e.Logger)\n",
       "\n",
       "    -\treturn errors.WithStack(newProcess(e).Start(ctx))\n",
       "    +\tvar wg sync.WaitGroup\n",
       "    +\tfor i := 0; i < e.Config.Concurrency; i++ {\n",
       "    +\t\twg.Add(1)\n",
       "    +\t\tgo func() {\n",
       "    +\t\t\tdefer wg.Done()\n",
       "    +\t\t\terr := errors.WithStack(newProcess(e).Start(ctx))\n",
       "    +\t\t\tif err != nil {\n",
       "    +\t\t\t\te.Logger.Printf(\"Error in the process: %v\", err)\n",
       "    +\t\t\t}\n",
       "    +\t\t}()\n",
       "    +\t}\n",
       "    +\twg.Wait()\n",
       "    +\treturn nil\n",
       "    }\n",
       "    ```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "issue = Path(\"/Users/leung/Workspace/personal/sherlock/subee/issues/test.md\")\n",
    "code = Path(\"/Users/leung/Workspace/personal/sherlock/subee\")\n",
    "\n",
    "analysis = run_chain(issue, code)\n",
    "\n",
    "pprint.pprint(object=analysis, indent=2)\n",
    "\n",
    "code = codegen_from_pr(issue, analysis, code)\n",
    "\n",
    "pprint.pprint(object=code, indent=2)\n",
    "\n",
    "\n",
    "final_pr = generate_final_pr(issue, analysis, code)\n",
    "display(Markdown(final_pr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sherlock-Ob8JdXDv-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
